{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning & AI - Spring 2019\n",
    "\n",
    "### Resources\n",
    "  - Announcements:  **Moodle**\n",
    "  - Weeklies:       **GitHub**\n",
    "  - Slides:         **GitHub**\n",
    "  - Code:           **Github**\n",
    "\n",
    "<center>Jens Egholm Pedersen & Jacob Trier Frederiksen</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 06\n",
    "\n",
    "### Focus this week\n",
    "##### Create a Common Platform(!) -- or make sure to be on the same page, at least.\n",
    "- VirtualBox + Ubuntu18 (desktop ed.) + Jupyter + Python3 & libs.\n",
    "\n",
    "##### Content: \n",
    "- **The ML wheel, again**\n",
    "- **Classes of machine learning**\n",
    "  - How to devise an ML Model?\n",
    "- **Supervised Learning**\n",
    "  - Thinking about data, not computers\n",
    "  - Linear Regression, simple least squares regression\n",
    "  - Binary Classification, support vector machines\n",
    "- **Hands-on Exercises in groups**\n",
    "  - Linear algebra, brush-up. Run the Jupyter Notebook . NB: don't go beyond the notebook's step In[74].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classroom Activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activity 1 -- Infrastructure\n",
    "Is everyone up-to-speed? Checking up on:\n",
    "  * **Python** devel environment,\n",
    "  * **Jupyter Notebooks** working installation,\n",
    "  * **Github** access and usage,\n",
    "  * **Peergrade** access and usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Activity 2 -- Mentimeter\n",
    "- Where's the comfort zone?\n",
    "- Workshop hours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Activitiy 3 -- Review Week 5\n",
    "- Student Q & A Session on last week's introduction.\n",
    "- Peer-driven review & discussion of Deep-Dream example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are we on the same page?\n"
     ]
    }
   ],
   "source": [
    "print('Are we on the same page?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The ML wheel, again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**This time, we break a part off the wheel...**\n",
    "<center><img src = \"images/MLWheelOurs.png\" style=\"width:50%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## _Classes_ of Machine Learning\n",
    "\n",
    "1. ##### Supervised Learning\n",
    "2. ##### Un-Supervised Learning\n",
    "3. ##### Semi-supervised\n",
    "4. ##### Reinforcement Learning\n",
    "5. ##### ... others [+ <unknown future variants ?]\n",
    "\n",
    "## _Algorithms_ of Machine Learning\n",
    "\n",
    "1. ##### Ordinary Least Squares Regression\n",
    "1. ##### Support Vector Machines\n",
    "2. ##### Principal Component Analysis\n",
    "3. ##### K-Means Clustering\n",
    "4. ##### Convolutional Neural Nets\n",
    "5. ##### Autoencoding Neural Nets\n",
    "6. ##### ... others [+ unknown future variants ?]\n",
    "\n",
    "## Exercise: team up in groups of 3-4.\n",
    "### Each group find the _path_ through the following chart (next page)  to find the Student Dropout/no Dropout scenario. You describe, we discuss. -- We save the algorithms for the course :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How should I design my ML model?**\n",
    "<center><img src = \"images/MLPaths.png\" style=\"width:70%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "#### Linear Regression \n",
    "- Linear\n",
    "  - House prices, consumer age, personal income\n",
    "  - Estimate a linear function from a set of real-valued data\n",
    "\n",
    "#### Classification\n",
    "- Binary\n",
    "  - Sick/Healthy, dead/alive, not cat/cat, 0/1, dropout/stay\n",
    "- Multiclass\n",
    "  - Nationalities, species, integers, vehicle types (bus/car/train/bicycle), letters of the alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Making a model\n",
    "* Minimize the distance of a function from a set of labelled feature vectors. Simple example is linear fit.\n",
    "* Very crude linear model for _life satisfaction by GDP per capita_, in some countries: $S = Q_{GDP} \\cdot {GDP} + S_{avg}$\n",
    "* Remember your $f(x)=ax + b$  ?\n",
    "\n",
    "<center><img src = \"images/GDPModel_Data.png\" style=\"width:50%\"></center>\n",
    "\n",
    "#### Q: What would you guess from the data for a fitting function? Linear? Quadratic? Cubic? ...\n",
    "#### Q: Are data representative? Yes? No? Why? Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "#### Making a model\n",
    "* Minimize the distance of a function from a set of labelled feature vectors. Simple example is linear fit.\n",
    "* Very crude linear model for _life satisfaction by GDP per capita_, in some countries: $S = Q_{GDP} \\cdot {GDP} + S_{avg}$\n",
    "* Remember your $f(x)=ax + b$ ?\n",
    "\n",
    "<center><img src = \"images/GDPModel_LinearFreedom.png\" style=\"width:50%\"></center>\n",
    "\n",
    "#### Q: Are we close to a reasonble choice of function (linear)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "#### Making a model\n",
    "* Minimize the distance of a function from a set of labelled feature vectors. Simple example is linear fit.\n",
    "* Very crude linear model for _life satisfaction by GDP per capita_, in some countries: $S = Q_{GDP} \\cdot {GDP} + S_{avg}$ \n",
    "* Remember your $f(x)=ax + b$ ?\n",
    "\n",
    "<center><img src = \"images/GDPModel_OptimalFit.png\" style=\"width:50%\"></center>\n",
    "\n",
    "#### Q: This looks like an optimal fit -- what could possibly go wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "#### Making a model\n",
    "* Minimize the distance of a function from a set of labelled feature vectors. Simple example is linear fit.\n",
    "* Very crude linear model for _life satisfaction by GDP per capita_, in some countries: $S = Q_{GDP} \\cdot {GDP} + S_{avg}$\n",
    "* Remember your $f(x)=ax + b$ ?\n",
    "\n",
    "<center><img src = \"images/GDPModel_MoreData.png\" style=\"width:50%\"></center>\n",
    "\n",
    "#### Q: What do you think from this (new) result, based on more data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "#### Making a model\n",
    "* Minimize the distance of a function from a set of labelled feature vectors. Simple example is linear fit.\n",
    "* Very crude linear model for _life satisfaction by GDP per capita_, in some countries: $S = Q_{GDP} \\cdot {GDP} + S_{avg}$\n",
    "* Remember your $f(x)=ax + b$ ?\n",
    "\n",
    "<center><img src = \"images/GDPModel_OverFit.png\" style=\"width:50%\"></center>\n",
    "\n",
    "Here is a very high order polynomial fit to data: $f(x) = a_1 x + a_2 x{^2} + a_3 x{^3} + a_4 x{^4} \\ldots + a_{10} x{^{10}} + b$.\n",
    "\n",
    "#### Q: Why is this model unreasonable in effect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "#### Simplest example is 1-D (dimensional) feature vectors, with a single real-valued label (the function value)\n",
    "* Minimize the distance of an assumed function from a set of labelled feature vectors. \n",
    "\n",
    "Generally: \n",
    "- a feature vector ${\\bf x}$ (x-values of the data point), a real-valued label $y$ (function value of data point).\n",
    "- a labelled feature vector $({\\bf x},y)$ (a data point).\n",
    "- N labelled feature vectors $\\{({\\bf x}_i,y_i)\\}_1^N$ (a data set).\n",
    "\n",
    "In 1D:\n",
    "- a feature vector $x$ (just one real valued number), a label $y$ (y-value of feature vector).\n",
    "- a labelled feature vector $(x, y)$ (a data point).\n",
    "- N labelled feature vectors $\\{(x_i, y_i)\\}_{i=1}^N$ (a data set).\n",
    "\n",
    "\n",
    "* Based on the _known labelled_ examples (data points): minimize the total distance between the line (function fit) and the _known labelled_ data points. \n",
    "* _Assume_ a linear relation -- which means... ? Polynomial of degree 1: $f(x) = ax + b$.\n",
    "\n",
    "\"Learning\" the \"correct\" function means minimizing a _**Loss Function**_. Our loss function in 1D is $$L = \\frac{1}{N} \\sum_{i=1,\\ldots,N} (f_{a,b}(x_i) - y_i)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression -- Exercise\n",
    "\n",
    "#### Part 1 -- A Simple Fit, straight line\n",
    "1. Go to the SciPy [Linear Regression Examples Page](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression), and download/run the simple **_'Linear Regression Example'_**.\n",
    "\n",
    "2. Talk about example in your groups. -- Was this machine learning? What did you achieve concerning predictive strength?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression -- Exercise\n",
    "\n",
    "#### Part 2 -- Underfitting / Overfitting\n",
    "1. Go to the SciPy [Linear Regression Examples Page](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression), and download/run the **_'Underfitting vs. Overfitting'_**.\n",
    "\n",
    "2. Talk about example in your groups. -- consider the challenges inherent in the _data_ rather than the learning (fitting) technique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TakeAway from Linear Regression?\n",
    "\n",
    "<center><img src = \"images/BadDataBadAlgo.png\" style=\"width:100%\"></center>\n",
    "\n",
    "#### Add to that: _\"... bad model assumptions.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Exercise:\n",
    "- In groups(!), find example of an ML case involving binary classification. \n",
    "- Discuss cases in plenum the following:\n",
    "  - the **_business case_**\n",
    "  - data and **_features_**\n",
    "  - choice **_ML class_**\n",
    "  - choice of **_ML algorithm_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### From a set of _labelled feature vectors_, labels being of binary nature, learn to separate the two classes _maximally_ by a line -- or other shape in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<left><img src = \"images/svm_try.png\" style=\"width:50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### From a set of _labelled feature vectors_, labels being of binary nature, learn to separate the two classes _maximally_ by a line -- or other shape in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<left><img src = \"images/svm_found.png\" style=\"width:50%\"></left>\n",
    "\n",
    "The middle of the \"road\" is called a 'decision surface'.\n",
    "\n",
    "Should be as far from both classes as possible. **Maximize width of the \"road\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Kernel Trick\n",
    "Harder cases.... later in the course, but for a taster...:\n",
    "<center><img src = \"images/kernel_trick.png\" style=\"width:75%\"></center>\n",
    "If we can construct a function (something like $f(x_i, x_j) \\equiv \\phi(x_i)\\phi(x_j)$, to add a dimension to our data -- that just might \"lift\" the degeneracy and reveal nicely linearly separated classes in a higher dimensional space. The is called the kernel trick, and it is magic, no less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary Classification -- Exercise\n",
    "\n",
    "1. With your team(!), choose one of the following from the [Hands-on Python book](https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb):\n",
    "  - _'Titanic'_      difficulty = xx\n",
    "  - _'Spam'_         difficulty  = xxxx\n",
    "  - _'Not-5'_        difficulty  = xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
