{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RECAP!!! \n",
    "## Binary Classifiers -- Measuring their predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**What are the possible prediction outcomes about a binary class dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We have a 'ground truth', or simply a truth, and we have predictions.\n",
    "\n",
    "* Values of predictions can be: **'true', 'false'** (or 1,0 or a,b or ...)\n",
    "* Values of 'ground truth' can be: **'positive', 'negative'** (or 1,0 or a,b or ...), to distiguish from true/false.\n",
    "\n",
    "This gives us the following combinations of ground truth and prediction:\n",
    "\n",
    "* TP = true positives\n",
    "* TN = true negatives\n",
    "* FP = false positives\n",
    "* FN = false negatives\n",
    "\n",
    "Hence.... a measure of how confused our model is when it sees new data **fx validation data**. Now let's draw it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Derived quantities from confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Accuracy, defined as $\\frac{TP + TN}{TP + TN + FP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Fallout (ROC), defined as $\\frac{TN}{TN + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Precision (P-R), defined as $\\frac{TP}{TP + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Recall (ROC & P-R), defined as $\\frac{TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are _many_ more, but these are the most central derived measures of quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A more intuitive way to look at it is this\n",
    "![confusion](images/confusion/accuracy_precision_recall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extending to Multi-level Classifiers, the Iris Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Popquiz: What would the confusion matrix look like for the Iris data set??\n",
    "**Not that I'll tell, but if you stick around you produce it yourselves... :-)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Exercise\n",
    "**Go to the 'Iris_confusion_matrix.ipynb' notebook in the Week11 folder on GitHub, do the exercise there, together with your group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multi-class Confusion Matrices\n",
    "\n",
    "![ICFM](images/confusion/threelevelconfusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Time for Another Demonstration!\n",
    "### Multi-class Classifiers -- Measuring predictive power on Iris data.\n",
    "![AzureRule(d)](images/confusion/azure_ml_labs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Receiver Operator Characteristic, ROC\n",
    "![ROC](images/confusion/roccurve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision-Recall Curve, P-R\n",
    "![ROC](images/confusion/precisionrecallcurve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What to use - when?\n",
    "\n",
    "* The ROC validation is better for _balanced_ (or equal) numbers of each class\n",
    "* The P-R validation is better for moderate(-to-high) class _imbalance_.\n",
    "\n",
    "The ROC curve gives too optimistic a validation result, when classes are not in balance. \n",
    "##### Exercise: Can you explain why this might be? Any means, but you can start with a fig:\n",
    "![rocprtest](images/confusion/exercise_roc_vs_PR.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What to use - when?\n",
    "_\"If the proportion of positive to negative instances changes in a test set, the ROC curves will not change.\"_ \n",
    "\n",
    "_\"Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not.\"_ \n",
    "\n",
    "_\"ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so do not depend on class distributions.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![ROC_class_separation](images/confusion/ROC_class_separation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
