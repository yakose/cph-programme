{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An Important Lesson From History\n",
    "![planes](images/confusion/planes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Binary Classifiers -- Measuring their predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**What are the possible prediction outcomes about a binary class dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We have a 'ground truth', or simply a truth, and we have predictions.\n",
    "\n",
    "* Values of predictions can be: **'true', 'false'** (or 1,0 or a,b or ...)\n",
    "* Values of 'ground truth' can be: **'positive', 'negative'** (or 1,0 or a,b or ...), to distiguish from true/false.\n",
    "\n",
    "This gives us the following combinations of ground truth and prediction:\n",
    "\n",
    "* TP = true positives\n",
    "* TN = true negatives\n",
    "* FP = false positives\n",
    "* FN = false negatives\n",
    "\n",
    "Hence.... a measure of how confused our model is when it sees new data **fx validation data**. Now let's draw it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Derived quantities from confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Accuracy, defined as $\\frac{TP + TN}{TP + TN + FP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Precision, defined as $\\frac{TP}{TP + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Recall (or sensitivity), defined as $\\frac{TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are _many_ more, but these are the most central derived measures of quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A more intuitive way to look at it is this\n",
    "![confusion](images/confusion/accuracy_precision_recall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Popquiz: What would the confusion matrix look like for the Iris data set??\n",
    "### Not that I'll tell, but if you stick around you produce it yourselves... :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise: goto the Iris_confusion_matrix notebook in the Week10 folder on GitHub, do the exercise there, together with your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Receiver operator curve, ROC\n",
    "![ROC](images/confusion/roccurve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision-Recall curve, P-R\n",
    "![ROC](images/confusion/precisionrecallcurve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What to use - when?\n",
    "\n",
    "* The ROC validation is better for _balanced_ (or equal) numbers of each class\n",
    "* The P-R validation is better for moderate(-to-high) class _imbalance_.\n",
    "\n",
    "The ROC curve gives too optimistic a validation result, when classes are not in balance. \n",
    "\n",
    "## Exercise: Can you explain why this might be? Any means, but you can start with a fig:\n",
    "![rocprtest](images/confusion/exercise_roc_vs_PR.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for Hands-on!!! \n",
    "### Multi-class Classifiers -- Measuring predictive power on Iris data.\n",
    "Go to the _'Iris_confusion_matrix.ipynb'_ in the Week10 GitHub repo, and play with it."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
